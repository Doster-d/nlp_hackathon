{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Q&A Bot Responses\n",
    "\n",
    "This notebook compares the performance of two Q&A bots using various metrics:\n",
    "- BERTScore (using DeepPavlov/rubert-base-cased model)\n",
    "- ROUGE-1 and ROUGE-2 scores\n",
    "\n",
    "We'll analyze and compare the answers from both bots against reference answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Path and Import Evaluator\n",
    "\n",
    "First, we'll set up the Python path to include our source directory and import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_path() -> None:\n",
    "    \"\"\"Add src directory to Python path.\"\"\"\n",
    "    src_path = str(Path.cwd().parent / 'src')\n",
    "    \n",
    "    if src_path not in sys.path:\n",
    "        sys.path.insert(0, src_path)\n",
    "        print(f\"Added {src_path} to Python path\")\n",
    "    \n",
    "    try:\n",
    "        import metrics\n",
    "        print(\"Successfully imported metrics module\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing metrics module: {e}\")\n",
    "        print(f\"Current sys.path: {sys.path}\")\n",
    "        raise\n",
    "\n",
    "setup_path()\n",
    "from metrics.evaluator import Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Q&A Data\n",
    "\n",
    "Load the Q&A pairs from our JSON file containing both bots' answers and reference answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qa_data(file_path: Path = None) -> list:\n",
    "    \"\"\"Load Q&A pairs with answers from both bots from JSON file.\"\"\"\n",
    "    if file_path is None:\n",
    "        file_path = Path.cwd().parent / 'dataset' / 'qa_pairs.json'\n",
    "    \n",
    "    logging.info(f\"Loading Q&A data from {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        return data['qa_pairs']\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading Q&A data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load the data\n",
    "qa_data = load_qa_data()\n",
    "qa_df = pd.DataFrame(qa_data)\n",
    "qa_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Bot Answers\n",
    "\n",
    "Now we'll evaluate the answers from both bots using our metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = Evaluator(bert_model_name='DeepPavlov/rubert-base-cased')\n",
    "\n",
    "# Calculate metrics for each bot\n",
    "bot1_results = []\n",
    "bot2_results = []\n",
    "\n",
    "for idx, row in qa_df.iterrows():\n",
    "    logging.info(f\"Processing pair {idx + 1}/{len(qa_df)}\")\n",
    "    \n",
    "    # Evaluate Bot 1\n",
    "    if row['bot1_answer'] and row['reference_answer']:\n",
    "        try:\n",
    "            scores1 = evaluator.evaluate_text_similarity(\n",
    "                row['bot1_answer'],\n",
    "                row['reference_answer']\n",
    "            )\n",
    "            logging.info(f\"Bot 1 scores for pair {idx}: {scores1}\")\n",
    "            \n",
    "            bert_score1 = scores1.get('bert_score', 0.0)\n",
    "            rouge_scores1 = scores1.get('rouge', {})\n",
    "            rouge1_f1 = rouge_scores1.get('rouge-1', {}).get('f1', 0.0)\n",
    "            rouge2_f1 = rouge_scores1.get('rouge-2', {}).get('f1', 0.0)\n",
    "            \n",
    "            bot1_results.append({\n",
    "                'question': row['question'],\n",
    "                'bert_score': bert_score1,\n",
    "                'rouge1': rouge1_f1,\n",
    "                'rouge2': rouge2_f1\n",
    "            })\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing Bot 1 pair {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Evaluate Bot 2\n",
    "    if row['bot2_answer'] and row['reference_answer']:\n",
    "        try:\n",
    "            scores2 = evaluator.evaluate_text_similarity(\n",
    "                row['bot2_answer'],\n",
    "                row['reference_answer']\n",
    "            )\n",
    "            logging.info(f\"Bot 2 scores for pair {idx}: {scores2}\")\n",
    "            \n",
    "            bert_score2 = scores2.get('bert_score', 0.0)\n",
    "            rouge_scores2 = scores2.get('rouge', {})\n",
    "            rouge1_f2 = rouge_scores2.get('rouge-1', {}).get('f1', 0.0)\n",
    "            rouge2_f2 = rouge_scores2.get('rouge-2', {}).get('f1', 0.0)\n",
    "            \n",
    "            bot2_results.append({\n",
    "                'question': row['question'],\n",
    "                'bert_score': bert_score2,\n",
    "                'rouge1': rouge1_f2,\n",
    "                'rouge2': rouge2_f2\n",
    "            })\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing Bot 2 pair {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Create DataFrames for both bots\n",
    "bot1_df = pd.DataFrame(bot1_results)\n",
    "bot2_df = pd.DataFrame(bot2_results)\n",
    "\n",
    "# Rename columns\n",
    "for df in [bot1_df, bot2_df]:\n",
    "    df.rename(columns={\n",
    "        'bert_score': 'BERTScore',\n",
    "        'rouge1': 'ROUGE-1',\n",
    "        'rouge2': 'ROUGE-2'\n",
    "    }, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Let's create visualizations to compare the performance of both bots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "plt.figure(figsize=(15, 6))\n",
    "metrics = ['BERTScore', 'ROUGE-1', 'ROUGE-2']\n",
    "\n",
    "# Prepare data for plotting\n",
    "plot_data = pd.DataFrame({\n",
    "    'Bot 1': bot1_df[metrics].mean(),\n",
    "    'Bot 2': bot2_df[metrics].mean()\n",
    "})\n",
    "\n",
    "# Create bar plot\n",
    "plot_data.plot(kind='bar')\n",
    "plt.title('Average Metrics Comparison Between Bots')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Results\n",
    "\n",
    "Let's look at the summary statistics and best performing answers for each bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bot 1 Summary Statistics:\")\n",
    "print(bot1_df[metrics].describe())\n",
    "print(\"\\nBot 2 Summary Statistics:\")\n",
    "print(bot2_df[metrics].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare best answers\n",
    "for metric in metrics:\n",
    "    print(f\"\\nBest answers by {metric}:\")\n",
    "    \n",
    "    # Bot 1 best\n",
    "    if not bot1_df.empty:\n",
    "        bot1_best_idx = bot1_df[metric].idxmax()\n",
    "        question = bot1_df.iloc[bot1_best_idx]['question']\n",
    "        print(f\"\\nBot 1 best ({metric}: {bot1_df.iloc[bot1_best_idx][metric]:.3f}):\")\n",
    "        print(f\"Question: {question}\")\n",
    "        qa_row = qa_df[qa_df['question'] == question].iloc[0]\n",
    "        print(f\"Answer: {qa_row['bot1_answer']}\")\n",
    "        print(f\"Reference: {qa_row['reference_answer']}\")\n",
    "    \n",
    "    # Bot 2 best\n",
    "    if not bot2_df.empty:\n",
    "        bot2_best_idx = bot2_df[metric].idxmax()\n",
    "        question = bot2_df.iloc[bot2_best_idx]['question']\n",
    "        print(f\"\\nBot 2 best ({metric}: {bot2_df.iloc[bot2_best_idx][metric]:.3f}):\")\n",
    "        print(f\"Question: {question}\")\n",
    "        qa_row = qa_df[qa_df['question'] == question].iloc[0]\n",
    "        print(f\"Answer: {qa_row['bot2_answer']}\")\n",
    "        print(f\"Reference: {qa_row['reference_answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Finally, let's save our results to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "output_dir = Path(\"results\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save metrics\n",
    "bot1_df.to_csv(output_dir / 'bot1_metrics.csv', index=False)\n",
    "bot2_df.to_csv(output_dir / 'bot2_metrics.csv', index=False)\n",
    "\n",
    "# Save plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "plot_data.plot(kind='bar')\n",
    "plt.title('Average Metrics Comparison Between Bots')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'bot_comparison.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
